{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06ed8de-95c7-4edc-a663-8ce01a895864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address     Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  172.26.0.3  279.24 KiB  16      100.0%            c729ec26-6c63-4636-bca3-88aaaf624a14  rack1\n",
      "UN  172.26.0.2  181.58 KiB  16      100.0%            69948696-e749-45c7-83f3-7bc61ae171ca  rack1\n",
      "UN  172.26.0.4  286 KiB     16      100.0%            d90054af-c316-4c12-94f3-9cbe644194c4  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebe431f-03c5-4ac0-b3b9-3c3aa8734abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98117c20-7186-4fe2-9a46-6de636bb9643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7c95245a2770>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"drop keyspace if exists weather\")\n",
    "cass.execute(\"\"\"\n",
    "create keyspace weather with\n",
    "replication = {'class': 'SimpleStrategy', 'replication_factor': 3};\n",
    "\"\"\")\n",
    "cass.execute(\"use weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6023e0-0dfe-44a6-ac37-a00091be22e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7c954c0cd7b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"\"\"\n",
    "CREATE TYPE station_record (tmin int, tmax int)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6136f46-2aad-4686-bb68-c3ddff6d5f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7c95245a2050>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"\"\"\n",
    "create table stations(\n",
    "    id TEXT,\n",
    "    name TEXT static,\n",
    "    date DATE,\n",
    "    record weather.station_record,\n",
    "    PRIMARY KEY(id, date)\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7629bf-1d00-4536-b3a4-55b3a0152fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE weather.stations (\\n    id text,\\n    date date,\\n    name text static,\\n    record station_record,\\n    PRIMARY KEY (id, date)\\n) WITH CLUSTERING ORDER BY (date ASC)\\n    AND additional_write_policy = '99p'\\n    AND bloom_filter_fp_chance = 0.01\\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\\n    AND cdc = false\\n    AND comment = ''\\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\\n    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\\n    AND memtable = 'default'\\n    AND crc_check_chance = 1.0\\n    AND default_time_to_live = 0\\n    AND extensions = {}\\n    AND gc_grace_seconds = 864000\\n    AND max_index_interval = 2048\\n    AND memtable_flush_period_in_ms = 0\\n    AND min_index_interval = 128\\n    AND read_repair = 'BLOCKING'\\n    AND speculative_retry = '99p';\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "cass.execute(\"describe table weather.stations\").one().create_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80971911-6d47-45f9-bde8-673f344ebfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e82784cb-e8e4-4e8f-b331-b36ce595a303;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      ":: resolution report :: resolve 1238ms :: artifacts dl 37ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   0   ||   18  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e82784cb-e8e4-4e8f-b331-b36ce595a303\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 18 already retrieved (0kB/36ms)\n",
      "24/04/19 03:52:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6a5455a-246e-40c2-959b-229b06972c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(count=1313)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df = spark.read.text(\"ghcnd-stations.txt\")\n",
    "\n",
    "#wisconsin_stations_df = stations_df.filter(stations_df.value.substr(0,2) == \"US\")\n",
    "wisconsin_stations = stations_df.collect()\n",
    "\n",
    "for row in wisconsin_stations:\n",
    "    state = row.value[38:40].strip()\n",
    "    if(state == 'WI'):\n",
    "        id = row.value[0:11].strip()\n",
    "        name = row.value[41:72].strip()\n",
    "        name = name.replace(\"'\", \"''\")\n",
    "        query = f\"INSERT INTO weather.stations (id, name) VALUES ('{id}', '{name}')\"\n",
    "        cass.execute(query)\n",
    "\n",
    "cass.execute(\"SELECT COUNT(*) FROM weather.stations\").one()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89d968fb-35f6-4568-8474-82f8bc1a19fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AMBERG 1.3 SW'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "result = cass.execute(\"SELECT name FROM weather.stations WHERE id = 'US1WIMR0003'\").one()\n",
    "result.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e15fd081-8731-49c5-82bf-60a9bf94ee77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "query = f\"SELECT token(id) FROM weather.stations WHERE id = '{'USC00470273'}'\"\n",
    "result = cass.execute(query)\n",
    "token = result.one()[0]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8768e910-2cb6-4a78-800e-16331ff24c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8530598149243383117"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "import subprocess\n",
    "ring = subprocess.check_output([\"nodetool\", \"ring\"]).decode(\"utf-8\")\n",
    "for line in ring.splitlines()[5:]:\n",
    "    node_id = line[75:]\n",
    "    if node_id and int(node_id) > token:\n",
    "        ret = int(node_id)\n",
    "        break\n",
    "if not ret:\n",
    "    ret = int(ring.splitlines()[5][75:])\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bee42777-cc0d-41af-bf1d-a3be82844a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  records.zip\n"
     ]
    }
   ],
   "source": [
    "!unzip -n records.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb0969ae-fa18-4bf7-9957-9c880ae57258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+------+\n",
      "|    station|    date|element| value|\n",
      "+-----------+--------+-------+------+\n",
      "|USW00014898|20220101|   TMAX| -32.0|\n",
      "|USW00014898|20220102|   TMAX| -77.0|\n",
      "|USW00014898|20220103|   TMAX| -60.0|\n",
      "|USW00014898|20220104|   TMAX|   0.0|\n",
      "|USW00014898|20220105|   TMAX| -16.0|\n",
      "|USW00014898|20220106|   TMAX| -71.0|\n",
      "|USW00014898|20220107|   TMAX| -71.0|\n",
      "|USW00014898|20220108|   TMAX| -32.0|\n",
      "|USW00014898|20220109|   TMAX| -27.0|\n",
      "|USW00014898|20220110|   TMAX|-149.0|\n",
      "|USW00014898|20220111|   TMAX| -16.0|\n",
      "|USW00014898|20220112|   TMAX|   6.0|\n",
      "|USW00014898|20220113|   TMAX|  11.0|\n",
      "|USW00014898|20220114|   TMAX| -77.0|\n",
      "|USW00014898|20220115|   TMAX| -99.0|\n",
      "|USW00014898|20220116|   TMAX| -60.0|\n",
      "|USW00014898|20220117|   TMAX| -21.0|\n",
      "|USW00014898|20220118|   TMAX|  28.0|\n",
      "|USW00014898|20220119|   TMAX|  28.0|\n",
      "|USW00014898|20220120|   TMAX|-121.0|\n",
      "|USW00014898|20220121|   TMAX| -77.0|\n",
      "|USW00014898|20220122|   TMAX| -43.0|\n",
      "|USW00014898|20220123|   TMAX|-110.0|\n",
      "|USW00014898|20220124|   TMAX| -71.0|\n",
      "|USW00014898|20220125|   TMAX|-138.0|\n",
      "|USW00014898|20220126|   TMAX|-116.0|\n",
      "|USW00014898|20220127|   TMAX| -16.0|\n",
      "|USW00014898|20220128|   TMAX| -77.0|\n",
      "|USW00014898|20220129|   TMAX| -60.0|\n",
      "|USW00014898|20220130|   TMAX| -21.0|\n",
      "|USW00014898|20220131|   TMAX|   6.0|\n",
      "|USW00014898|20220101|   TMIN|-110.0|\n",
      "|USW00014898|20220102|   TMIN|-166.0|\n",
      "|USW00014898|20220103|   TMIN|-171.0|\n",
      "|USW00014898|20220104|   TMIN|-116.0|\n",
      "|USW00014898|20220105|   TMIN| -88.0|\n",
      "|USW00014898|20220106|   TMIN|-127.0|\n",
      "|USW00014898|20220107|   TMIN|-166.0|\n",
      "|USW00014898|20220108|   TMIN|-127.0|\n",
      "|USW00014898|20220109|   TMIN|-171.0|\n",
      "|USW00014898|20220110|   TMIN|-232.0|\n",
      "|USW00014898|20220111|   TMIN|-227.0|\n",
      "|USW00014898|20220112|   TMIN| -27.0|\n",
      "|USW00014898|20220113|   TMIN| -77.0|\n",
      "|USW00014898|20220114|   TMIN|-121.0|\n",
      "|USW00014898|20220115|   TMIN|-193.0|\n",
      "|USW00014898|20220116|   TMIN|-199.0|\n",
      "|USW00014898|20220117|   TMIN| -66.0|\n",
      "|USW00014898|20220118|   TMIN| -32.0|\n",
      "|USW00014898|20220119|   TMIN|-155.0|\n",
      "+-----------+--------+-------+------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "weather_df = spark.read.parquet(\"records.parquet\")\n",
    "weather_df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8a9a3b8-a5c2-4602-bdd6-24f1a3b509fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "weather_df = weather_df.filter((col(\"element\") == \"TMIN\") | (col(\"element\") == \"TMAX\")) \\\n",
    "    .select(\"station\", \"date\", \"element\", \"value\") \\\n",
    "    .groupBy(\"station\", \"date\") \\\n",
    "    .pivot(\"element\") \\\n",
    "    .agg({\"value\": \"first\"}) \\\n",
    "    .withColumnRenamed(\"TMIN\", \"tmin\") \\\n",
    "    .withColumnRenamed(\"TMAX\", \"tmax\")\n",
    "\n",
    "weather_df = weather_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "\n",
    "for row in weather_df.collect():\n",
    "    station_id = row[\"station\"]\n",
    "    date = row[\"date\"]\n",
    "    tmin = row[\"tmin\"]\n",
    "    tmax = row[\"tmax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f37388f7-050a-4478-a43b-ddf6bd33f2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom station_pb2_grpc import StationStub\\nimport grpc\\nimport station_pb2\\n\\nserver_address = 'localhost:5440'  # Update with your server address\\nchannel = grpc.insecure_channel(server_address)\\nstub = StationStub(channel)\\n\\nstation_id = 'USW00014837'\\nmax_temp_response = stub.StationMax(station_pb2.StationMaxRequest(station=station_id))\\n\\nmax_temp_response\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "\"\"\"\n",
    "from station_pb2_grpc import StationStub\n",
    "import grpc\n",
    "import station_pb2\n",
    "\n",
    "server_address = 'localhost:5440'  # Update with your server address\n",
    "channel = grpc.insecure_channel(server_address)\n",
    "stub = StationStub(channel)\n",
    "\n",
    "station_id = 'USW00014837'\n",
    "max_temp_response = stub.StationMax(station_pb2.StationMaxRequest(station=station_id))\n",
    "\n",
    "max_temp_response\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46785c5-b0a1-4ab6-9b4d-9a4306df4a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/19 03:53:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Cassandra Spark Integration\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from Cassandra table into a DataFrame\n",
    "stations_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"weather\") \\\n",
    "    .option(\"table\", \"stations\") \\\n",
    "    .load()\n",
    "\n",
    "stations_df.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f94e043b-23fb-4043-9739-14372608adf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='stations', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d5df4b2-9cfe-421c-8d24-19f69d38bae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avg_temp_diff_df = stations_df \\\n",
    "    .filter(col(\"record\").isNotNull()) \\\n",
    "    .withColumn(\"temp_diff\", col(\"record.tmax\") - col(\"record.tmin\")) \\\n",
    "    .groupBy(\"id\") \\\n",
    "    .agg({\"temp_diff\": \"avg\"}) \\\n",
    "    .withColumnRenamed(\"avg(temp_diff)\", \"avg_temp_diff\")\n",
    "\n",
    "avg_temp_diff_dict = avg_temp_diff_df.rdd.map(lambda row: (row['id'], row['avg_temp_diff'])).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9281bf5-a49e-4e8f-9637-8fcbb136a014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "avg_temp_diff_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7f439a3-7e51-4a4b-8a55-7a8f12d99092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address     Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  172.26.0.3  226.44 KiB  16      100.0%            c729ec26-6c63-4636-bca3-88aaaf624a14  rack1\n",
      "UN  172.26.0.2  276.28 KiB  16      100.0%            69948696-e749-45c7-83f3-7bc61ae171ca  rack1\n",
      "UN  172.26.0.4  232.82 KiB  16      100.0%            d90054af-c316-4c12-94f3-9cbe644194c4  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fc35525-29c5-4366-b05e-ac7bb5c94e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nserver_address = 'localhost:5440'  # Update with your server address\\nchannel = grpc.insecure_channel(server_address)\\nstub = StationStub(channel)\\n\\nstation_id = 'USW00014837'\\nmax_temp_response = stub.StationMax(station_pb2.StationMaxRequest(station=station_id))\\n\\nmax_temp_response\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q9\n",
    "\"\"\"\n",
    "server_address = 'localhost:5440'  # Update with your server address\n",
    "channel = grpc.insecure_channel(server_address)\n",
    "stub = StationStub(channel)\n",
    "\n",
    "station_id = 'USW00014837'\n",
    "max_temp_response = stub.StationMax(station_pb2.StationMaxRequest(station=station_id))\n",
    "\n",
    "max_temp_response\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa58eed4-9a16-44b2-82d6-df69e05174ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nstation_id = 'USW00014839'\\ndate = '2024-04-18'\\ntmin = 10\\ntmax = 25\\n\\nresponse = stub.RecordTemps(station_pb2.RecordTempsRequest(\\n    station=station_id,\\n    date=date,\\n    tmin=tmin,\\n    tmax=tmax\\n        ))\\n\\nresponse.error\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q10\n",
    "\"\"\"\n",
    "station_id = 'USW00014839'\n",
    "date = '2024-04-18'\n",
    "tmin = 10\n",
    "tmax = 25\n",
    "\n",
    "response = stub.RecordTemps(station_pb2.RecordTempsRequest(\n",
    "    station=station_id,\n",
    "    date=date,\n",
    "    tmin=tmin,\n",
    "    tmax=tmax\n",
    "        ))\n",
    "\n",
    "response.error\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
